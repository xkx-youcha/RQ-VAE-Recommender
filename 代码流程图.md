# RQ-VAE æ¨èç³»ç»Ÿä»£ç æµç¨‹å›¾

## ğŸ”„ å®Œæ•´ç³»ç»Ÿæµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹] --> B{é€‰æ‹©è®­ç»ƒé˜¶æ®µ}
    
    B -->|ç¬¬ä¸€é˜¶æ®µ| C[RQ-VAEè®­ç»ƒ]
    B -->|ç¬¬äºŒé˜¶æ®µ| D[æ£€ç´¢æ¨¡å‹è®­ç»ƒ]
    
    %% RQ-VAEè®­ç»ƒæµç¨‹
    C --> C1[åŠ è½½ç‰©å“æ•°æ®]
    C1 --> C2[åˆå§‹åŒ–RQ-VAEæ¨¡å‹]
    C2 --> C3[è®­ç»ƒå¾ªç¯]
    C3 --> C4[ç‰©å“ç‰¹å¾ç¼–ç ]
    C4 --> C5[å¤šå±‚æ®‹å·®é‡åŒ–]
    C5 --> C6[ç”Ÿæˆè¯­ä¹‰ID]
    C6 --> C7[è®¡ç®—æŸå¤±]
    C7 --> C8[åå‘ä¼ æ’­]
    C8 --> C9{æ˜¯å¦å®Œæˆ?}
    C9 -->|å¦| C3
    C9 -->|æ˜¯| C10[ä¿å­˜æ¨¡å‹]
    
    %% æ£€ç´¢æ¨¡å‹è®­ç»ƒæµç¨‹
    D --> D1[åŠ è½½é¢„è®­ç»ƒRQ-VAE]
    D1 --> D2[åŠ è½½åºåˆ—æ•°æ®]
    D2 --> D3[åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹]
    D3 --> D4[è®­ç»ƒå¾ªç¯]
    D4 --> D5[åºåˆ—ç¼–ç ]
    D5 --> D6[Transformerè§£ç ]
    D6 --> D7[ç”Ÿæˆä¸‹ä¸€ä¸ªè¯­ä¹‰ID]
    D7 --> D8[è®¡ç®—æŸå¤±]
    D8 --> D9[åå‘ä¼ æ’­]
    D9 --> D10{æ˜¯å¦å®Œæˆ?}
    D10 -->|å¦| D4
    D10 -->|æ˜¯| D11[ä¿å­˜æ¨¡å‹]
    
    C10 --> E[ç»“æŸ]
    D11 --> E
```

## ğŸ“Š è¯¦ç»†ä»£ç æ‰§è¡Œæµç¨‹

### ç¬¬ä¸€é˜¶æ®µï¼šRQ-VAEè®­ç»ƒ (`train_rqvae.py`)

#### 1. åˆå§‹åŒ–é˜¶æ®µ
```python
# 1. é…ç½®è§£æ
@gin.configurable
def train(iterations=50000, batch_size=64, ...):
    # è§£æginé…ç½®æ–‡ä»¶å‚æ•°
    
# 2. åŠ é€Ÿå™¨åˆå§‹åŒ–
accelerator = Accelerator(
    split_batches=split_batches,
    mixed_precision=mixed_precision_type if amp else 'no'
)

# 3. æ•°æ®é›†åŠ è½½
train_dataset = ItemData(
    root=dataset_folder, 
    dataset=dataset, 
    force_process=force_dataset_process, 
    train_test_split="train" if do_eval else "all", 
    split=dataset_split
)

# 4. æ•°æ®åŠ è½½å™¨
train_dataloader = DataLoader(
    train_dataset, 
    sampler=train_sampler, 
    batch_size=None, 
    collate_fn=lambda batch: batch
)
```

#### 2. æ¨¡å‹åˆå§‹åŒ–
```python
# 5. RQ-VAEæ¨¡å‹åˆ›å»º
model = RqVae(
    input_dim=vae_input_dim,        # 768
    embed_dim=vae_embed_dim,        # 32
    hidden_dims=vae_hidden_dims,    # [512, 256, 128]
    codebook_size=vae_codebook_size, # 256
    codebook_kmeans_init=use_kmeans_init,
    codebook_normalize=vae_codebook_normalize,
    codebook_mode=vae_codebook_mode,
    n_layers=vae_n_layers,          # 3
    commitment_weight=commitment_weight # 0.25
)

# 6. ä¼˜åŒ–å™¨
optimizer = AdamW(
    params=model.parameters(),
    lr=learning_rate,
    weight_decay=weight_decay
)
```

#### 3. è®­ç»ƒå¾ªç¯
```python
# 7. ä¸»è®­ç»ƒå¾ªç¯
for iteration in tqdm(range(iterations)):
    # è·å–æ‰¹æ¬¡æ•°æ®
    batch = next_batch(train_dataloader)
    batch = batch_to(batch, device)
    
    # å‰å‘ä¼ æ’­
    with accelerator.autocast():
        losses = model(batch, gumbel_t=gumbel_t)
        loss = losses.loss
    
    # åå‘ä¼ æ’­
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
    
    # æ—¥å¿—è®°å½•
    if iteration % log_every == 0:
        log_metrics(losses, iteration)
    
    # æ¨¡å‹ä¿å­˜
    if iteration % save_model_every == 0:
        save_checkpoint(model, optimizer, iteration)
```

### ç¬¬äºŒé˜¶æ®µï¼šæ£€ç´¢æ¨¡å‹è®­ç»ƒ (`train_decoder.py`)

#### 1. åˆå§‹åŒ–é˜¶æ®µ
```python
# 1. åŠ è½½é¢„è®­ç»ƒRQ-VAE
if pretrained_rqvae_path:
    rqvae = RqVae.load_pretrained(pretrained_rqvae_path)
    rqvae.eval()

# 2. åˆ›å»ºè¯­ä¹‰IDåˆ†è¯å™¨
tokenizer = SemanticIdTokenizer(
    rqvae=rqvae,
    device=device
)

# 3. åŠ è½½åºåˆ—æ•°æ®
train_dataset = SeqData(
    root=dataset_folder, 
    dataset=dataset, 
    is_train=True, 
    subsample=train_data_subsample, 
    split=dataset_split
)
```

#### 2. æ¨¡å‹åˆå§‹åŒ–
```python
# 4. æ£€ç´¢æ¨¡å‹åˆ›å»º
model = EncoderDecoderRetrievalModel(
    embedding_dim=decoder_embed_dim,    # 128
    attn_dim=attn_embed_dim,           # 512
    dropout=dropout_p,                  # 0.3
    num_heads=attn_heads,              # 8
    n_layers=attn_layers,              # 8
    num_embeddings=vae_codebook_size,  # 256
    sem_id_dim=vae_n_layers,           # 3
    inference_verifier_fn=tokenizer.verify_semantic_ids
)
```

#### 3. è®­ç»ƒå¾ªç¯
```python
# 5. ä¸»è®­ç»ƒå¾ªç¯
for iteration in tqdm(range(iterations)):
    # è·å–æ‰¹æ¬¡æ•°æ®
    batch = next_batch(train_dataloader)
    batch = batch_to(batch, device)
    
    # åºåˆ—åˆ†è¯åŒ–
    tokenized_batch = tokenizer.tokenize_sequences(batch)
    
    # å‰å‘ä¼ æ’­
    with accelerator.autocast():
        model_output = model(tokenized_batch)
        loss = model_output.loss
    
    # åå‘ä¼ æ’­
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()
    
    # è¯„ä¼°
    if iteration % full_eval_every == 0:
        evaluate_model(model, tokenizer, eval_dataloader)
```

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è¯¦ç»†æµç¨‹

### RQ-VAEæ¨¡å— (`modules/rqvae.py`)

#### å‰å‘ä¼ æ’­æµç¨‹
```python
def forward(self, batch: SeqBatch, gumbel_t: float) -> RqVaeComputedLosses:
    # 1. ç¼–ç å™¨å¤„ç†
    x = batch.x  # [batch_size, input_dim]
    encoded = self.encoder(x)  # [batch_size, embed_dim]
    
    # 2. å¤šå±‚æ®‹å·®é‡åŒ–
    embeddings = []
    residuals = []
    sem_ids = []
    quantize_loss = 0
    
    current_input = encoded
    for i, layer in enumerate(self.layers):
        # é‡åŒ–å½“å‰å±‚
        quantized, loss = layer(current_input, gumbel_t)
        embeddings.append(quantized)
        residuals.append(current_input - quantized)
        
        # è·å–è¯­ä¹‰ID
        sem_id = layer.get_codebook_indices(current_input)
        sem_ids.append(sem_id)
        
        quantize_loss += loss
        
        # å‡†å¤‡ä¸‹ä¸€å±‚è¾“å…¥
        if i < len(self.layers) - 1:
            current_input = residuals[-1]
    
    # 3. è§£ç å™¨é‡æ„
    reconstructed = self.decoder(embeddings[-1])
    
    # 4. è®¡ç®—æŸå¤±
    reconstruction_loss = self.reconstruction_loss(reconstructed, x)
    total_loss = reconstruction_loss + quantize_loss
    
    return RqVaeComputedLosses(
        loss=total_loss,
        reconstruction_loss=reconstruction_loss,
        rqvae_loss=quantize_loss,
        embs_norm=embeddings[-1].norm(dim=-1).mean(),
        p_unique_ids=self._compute_unique_ids_ratio(sem_ids)
    )
```

### æ£€ç´¢æ¨¡å‹æ¨¡å— (`modules/model.py`)

#### å‰å‘ä¼ æ’­æµç¨‹
```python
def forward(self, batch: TokenizedSeqBatch) -> ModelOutput:
    # 1. åµŒå…¥å±‚å¤„ç†
    sem_id_embeddings = self.sem_id_embedder(batch.sem_ids)
    user_embeddings = self.user_id_embedder(batch.user_ids)
    
    # 2. ä½ç½®ç¼–ç 
    position_embeddings = self.wpe(batch.positions)
    
    # 3. è¾“å…¥æŠ•å½±
    input_embeddings = sem_id_embeddings + user_embeddings + position_embeddings
    projected_input = self.in_proj(input_embeddings)
    
    # 4. Transformerç¼–ç -è§£ç 
    if self.jagged_mode:
        # ä½¿ç”¨è‡ªå®šä¹‰Transformer
        encoded = self.transformer.encode(projected_input)
        decoded = self.transformer.decode(encoded, projected_input)
    else:
        # ä½¿ç”¨æ ‡å‡†Transformer
        encoded = self.transformer.encoder(projected_input)
        decoded = self.transformer.decoder(projected_input, encoded)
    
    # 5. è¾“å‡ºæŠ•å½±
    logits = self.out_proj(decoded)
    
    # 6. è®¡ç®—æŸå¤±
    loss = F.cross_entropy(
        logits.view(-1, logits.size(-1)), 
        batch.target_sem_ids.view(-1)
    )
    
    return ModelOutput(
        loss=loss,
        logits=logits,
        loss_d=loss.detach()
    )
```

## ğŸ“ˆ æ•°æ®æµè½¬å›¾

### ç‰©å“æ•°æ®å¤„ç†æµç¨‹
```mermaid
graph LR
    A[åŸå§‹ç‰©å“æ•°æ®] --> B[ç‰¹å¾æå–]
    B --> C[æ•°æ®é¢„å¤„ç†]
    C --> D[æ‰¹æ¬¡åŒ–]
    D --> E[è®¾å¤‡è½¬ç§»]
    E --> F[RQ-VAEè¾“å…¥]
```

### åºåˆ—æ•°æ®å¤„ç†æµç¨‹
```mermaid
graph LR
    A[ç”¨æˆ·è¡Œä¸ºåºåˆ—] --> B[ç‰©å“ç‰¹å¾æå–]
    B --> C[è¯­ä¹‰IDæ˜ å°„]
    C --> D[åºåˆ—æ„å»º]
    D --> E[åˆ†è¯åŒ–]
    E --> F[æ£€ç´¢æ¨¡å‹è¾“å…¥]
```

## ğŸ¯ å…³é”®å‡½æ•°è°ƒç”¨é“¾

### RQ-VAEè®­ç»ƒè°ƒç”¨é“¾
```
train_rqvae.py:train()
â”œâ”€â”€ ItemData.__init__()
â”œâ”€â”€ RqVae.__init__()
â”œâ”€â”€ train_loop:
â”‚   â”œâ”€â”€ next_batch()
â”‚   â”œâ”€â”€ RqVae.forward()
â”‚   â”‚   â”œâ”€â”€ MLP.forward() (ç¼–ç å™¨)
â”‚   â”‚   â”œâ”€â”€ Quantize.forward() (å¤šå±‚é‡åŒ–)
â”‚   â”‚   â”œâ”€â”€ MLP.forward() (è§£ç å™¨)
â”‚   â”‚   â””â”€â”€ lossè®¡ç®—
â”‚   â”œâ”€â”€ backward()
â”‚   â””â”€â”€ optimizer.step()
â””â”€â”€ save_checkpoint()
```

### æ£€ç´¢æ¨¡å‹è®­ç»ƒè°ƒç”¨é“¾
```
train_decoder.py:train()
â”œâ”€â”€ RqVae.load_pretrained()
â”œâ”€â”€ SemanticIdTokenizer.__init__()
â”œâ”€â”€ EncoderDecoderRetrievalModel.__init__()
â”œâ”€â”€ train_loop:
â”‚   â”œâ”€â”€ next_batch()
â”‚   â”œâ”€â”€ SemanticIdTokenizer.tokenize_sequences()
â”‚   â”œâ”€â”€ EncoderDecoderRetrievalModel.forward()
â”‚   â”‚   â”œâ”€â”€ SemIdEmbedder.forward()
â”‚   â”‚   â”œâ”€â”€ UserIdEmbedder.forward()
â”‚   â”‚   â”œâ”€â”€ Transformer.forward()
â”‚   â”‚   â””â”€â”€ lossè®¡ç®—
â”‚   â”œâ”€â”€ backward()
â”‚   â””â”€â”€ optimizer.step()
â””â”€â”€ save_checkpoint()
```

## ğŸ” è°ƒè¯•å’Œç›‘æ§ç‚¹

### 1. æ•°æ®æ£€æŸ¥ç‚¹
- ç‰©å“ç‰¹å¾ç»´åº¦: `[batch_size, 768]`
- è¯­ä¹‰IDç»´åº¦: `[batch_size, 3]`
- åºåˆ—é•¿åº¦: æœ€å¤§20 (Amazon) æˆ– 200 (MovieLens)

### 2. æ¨¡å‹æ£€æŸ¥ç‚¹
- RQ-VAEé‡æ„æŸå¤±: åº”è¯¥é€æ¸ä¸‹é™
- é‡åŒ–æŸå¤±: åº”è¯¥ä¿æŒç¨³å®š
- æ£€ç´¢æ¨¡å‹æŸå¤±: åº”è¯¥é€æ¸ä¸‹é™

### 3. æ€§èƒ½ç›‘æ§
- GPUå†…å­˜ä½¿ç”¨
- è®­ç»ƒé€Ÿåº¦ (iterations/second)
- è¯„ä¼°æŒ‡æ ‡ (Top-Kå‡†ç¡®ç‡)

---

*è¿™ä¸ªæµç¨‹å›¾è¯¦ç»†å±•ç¤ºäº†RQ-VAEæ¨èç³»ç»Ÿçš„å®Œæ•´ä»£ç æ‰§è¡Œè¿‡ç¨‹ï¼Œå¸®åŠ©ä½ ç†è§£æ¯ä¸ªæ¨¡å—çš„ä½œç”¨å’Œæ•°æ®æµè½¬ã€‚* 